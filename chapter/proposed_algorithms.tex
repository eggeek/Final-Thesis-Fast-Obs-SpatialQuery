\chapter{Proposed Algorithms}\label{proposedalgo}
\section{Overview}\label{proposedoverview}
From the previous chapter, we have introduced a very fast point-to-point algorithm \textit{Polyanya}, 
in this chapter, we discuss how to effectively adapt \textit{Polyanya} for OkNN settings where
there are multiple candidate targets. In section~\ref{prob}, we introduce formal problem
statement and math notations; in section~\ref{mot}, we introduce two less efficient but very
straightforward solution to show the motivations of our proposed research;
section~\ref{intervalh} and \ref{targeth} present our research works which discuss the design of our algorithms and the correctness in theory.

\section{Problem Statement}\label{prob}
OkNN is a spatial query in two dimensions that can be formalised as follows:
\begin{definition}{Obstacle k-Nearest Neighbour (OkNN):}\newline
Given a set of points $T$, a set of obstacles $O$, a distinguished point $q$ and and an integer $k$: 
\textbf{return} a set $\text{kNN} = \{t | t \in T\}$ such that $d_o(q, t) \le d_o(q, t_k)$ for all $t \in \text{kNN}$.
\end{definition}

\noindent Where:
\begin{itemize}[leftmargin=+1cm]
\item $O$ is a set of non-traversable polygonal obstacles.
\item $T$ is a set of traversable points called \emph{targets}.
\item $q$ is a traversable point called the \emph{query point}.
\item $k$ is an input parameter that controls the number of nearest neighbours that will be returned.
\item $d_e$ and $d_o$ are functions that measure the shortest distance between two points, as discussed below.
\item $t_k$ is the $k^{th}$ nearest neighbour of $q$.
\item $h_p(n, t)$ is the \textit{h-value} in Polyanya for a given search node $n$ and a target
  $t$.
\end{itemize}
\noindent
Stated in simple words, the objective is to find the set of $k$ targets which are closest to $q$ from among all possible candidates in $T$.
When discussing distances between two points $q$ and $t$ we distinguish between two metrics:
$d_e(q, t)$ which is the well known Euclidean metric (i.e. ``straight-line distance'')
and $d_o(q, t)$ which measures the length of a shortest path $\pi_{q, t} = \langle q, \ldots, t\rangle$ between points $q$ and $t$ such that
no pairwise segment of the path intersects any point inside an obstacle (i.e. ``obstacle avoiding distance'').


\section{Motivation}\label{mot}
Since \textit{Polyanya} instantiates \textit{A*} search and since that algorithm is itself a
special case of \textit{Dijkstra}'s well know technique, there exists a simple modification at
hand: we can simply remove the influence of the cost-to-go heuristic and allow the search to
continue until it has expanded the $k^{th}$ target, let's call this \textbf{\textit{zero-heuristic}}.
All other aspects of the algorithm, including termination
\footnote{There are two cases to consider depending on whether the query and target points are in the same polygon or in different polygons.
Both are described in~\cite{cuicompromise}}, remain unchanged.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\linewidth]{pic/suc.png}
    \caption{\small Example of successors from \cite{cuicompromise}.}
    \label{suc2}
\end{figure}

The version of \textit{Polyanya} we have just described is unlikely to be efficient. Without a
\textit{h-value} for guidance, nodes can only be prioritized by the \textit{g-value} of their
root point, which is settled at the time of expansion. However, the \textit{g-value} does not
reflect the distance between the root and its corresponding interval. For example, in
Figure~\ref{suc2}, all observable successors would have the same expansion priority. Thus we
may expand many nodes, all equally priomising but having distant intervals, and all before
reaching a nearby target node with a slightly higher \textit{g-value}.

Another naive adaption is repeatedly calling an unmodified point-to-point \textit{Polyanya}
search, from the query point and to each target, let's call this \textbf{\textit{brute-force
Polyanya}}, see in algorithm~\ref{brute}. It is obvious that this solution is
inefficient when targets are many, however, in chapter\ref{empirical} we will see it
outperforms other proposed algorithms in certain contexts. 
\begin{algorithm}[!htp]
  \input{./code/brute_polyanya.pseudo}
  \caption{Brute-force Polyanya}
  \label{brute}
\end{algorithm}
To deal with this problem we develop two online heuristics which can be fruitfully applied to OkNN:
\begin{itemize}
  \item The Interval Heuristic, which prioritizes nodes using the closest point from its
    associated interval.
  \item The Target Heuristic, which relies on a Euclidean nearest neighbour estimator to
    provide a target dynamically at the time of expansion.
\end{itemize}
Each of these heuristic is applied in the usual way compute a final expansion priority: 
$f(n) = \textit{g-value}(n) + \textit{h-value}(n)$. In the remainder of this chapter we explore
these ideas in turn.

\section{Interval Heuristic ($h_v$)}\label{intervalh}
In some OkNN settings targets are myriad and one simply requires a fast algorithm to explore
the local area. This approach is in contrast to more sophisticated methods which apply spatial
reasoning to prune the set of candidates. The idea we introduce for such settings is simple and
can be formalised as follows:

\begin{definition}
  Given search node $n=[I, r]$, the \textit{h-value} in Interval Heuristic $h_v(n)$ 
  is the minimal Euclidean distance from the root $r$ to the segment $I$.
\end{definition}

Applying the Interval Heuristic $h_v$ requires solving a simple geometric problem: finding the
closest point on a line. The operation has low constant time complexity and we apply
standard techniques. Algorithm~\ref{intervalsrc} shows an example.

\begin{algorithm}[!htb]
  \input{./code/interval.pseudo}
  \caption{Polyanya OkNN with interval heuristic}
  \label{intervalsrc}
\end{algorithm}

\begin{theorem}{\textbf{consistency of $h_v$}:}\label{nodescv}
  In interval heuristic, the $\textit{f-value}$ of successor node is not less than the $\textit{f-value}$ of the parent
  search node.
\end{theorem}

\begin{proof}
  In the interval heuristic, when the successor is a search node, its \textit{f-value}
  can be interpreted as a lower-bound of the length of path from
  $s$ to any point on the segment $I$ through root $r$, and since it is generated by pushing
  away from the parent search node, its \textit{f-value} is larger than \textit{f-value}
  of the parent search node. If successor is a target node, the \textit{f-value} is the the length
  of the corresponding path and not less than the parent search node (the two values are equal 
  when the target is on the segment $I$).%$\square$
\end{proof}

\begin{corollary}
  Expanding a target node corresponds to finding a shortest path.
\end{corollary}

\begin{proof}
  As per Theorem~\ref{nodescv}, when a final node is expanded there exists
  no remaining candidate on the open list which can reach the node with a smaller $f$-value.
\end{proof}

\section{Target Heuristic ($h_t$)}\label{targeth}
In some OkNN settings the set of target are few (i.e. sparse), or there is a filter on the
query, for example, the query is like "the nearest storage location where capacity $>=100$". 
In these cases, without a reasonable heuristic guide, it is possible to perform many redundant
expansions in areas where no nearest neighbor can exist, Figure~\ref{hv} shows an example. 
\begin{figure}[htp]
  \centering
  \begin{tikzpicture}[scale=0.8]
    \input{src/polyanya.tex}
    \intervalexpansion
  \end{tikzpicture}
  \caption{\small Search space of Interval Heuristic:$q$ is query point,
  $t$ is a target, green dashed segments are the interval of expanded search nodes.
  From the figure we can see that the algorithm does unnecessary expansions in the direction that no target.}
  \label{hv}
\end{figure}
\noindent
In such cases more sophisticated spatial reasoning can help to prune the set of nearest
neighbours and guide the search. The idea we introduce for such settings can be formalised as
follows:
\begin{definition}\label{close}
  The closest target $t$ of a search node $n$ is $t \in T$ where $h_p(n, t)$ is minimum,
  the \textit{h-value} of search node $n$ in target heuristic is $h_t(n)=h_p(n,t)$.
\end{definition}

\begin{theorem}{\textbf{consistency of $h_t$}:}\label{nodesct}
In target heuristic, the $\textit{f-value}$ of successor node is not less than the $\textit{f-value}$ of the parent search node.
\end{theorem}

\begin{proof}
  Let the search node be $n=(I, r)$, the successor be $n'=(I', r')$, closest target of $n$
  be $t$, and the closest target of $n'$ be $t'$. 
  So we have $\textit{f-value}(n) = \textit{g-value}(r) + h_p(n, t)$, and 
  $\textit{f-value}(n') = g(r) + d_e(r, r') + h_p(n', t')$.
  According to definition of successor\cite{cuicompromise}, we have $r'=r$ or $r' \in I$.
  There are two cases:
  \begin{itemize}
    \item If $t=t'$, according to definition of $h_p$\cite{cuicompromise},
      we have $d_e(r, r') + h_p(n', t) >= h_p(n, t)$. 
    \item If $t,t'$ are different, according to definition~\ref{close},
      $h_p(n, t) <= h_p(n, t') <= d_e(r, r') + h_p(n', t')$.
  \end{itemize}
  Thus, $\textit{f-value}(n') >= \textit{f-value}(n)$.
\end{proof}

We implement this idea as follow: 
When the relative location between targets and $r$ are in case 3 of the $h_p$,
instead of flipping targets, we flip $r$, and thus formed six areas as shown in Figure~\ref{fa}.
For $t \in areaA$, the \textit{h-value} is $d_e(r, a) + d_e(a, t)$;
for $t \in areaA'$, the \textit{h-value} is $d_e(r',a) + d_e(a, t)$,
and because $d_e(r', a) = d_e(r, a)$, we can combine $areaA$ and $areaA'$,
so we need to find the nearest neighbor of $a$ for $t \in areaA \cup areaA'$;
by the same reason, we can combine $areaB, areaB'$, so finally we formed four areas.
Then following the example, we may reason as follows:
\begin{figure*}[!hbt]
  \centering
  \includegraphics[width=.7\linewidth]{pic/heuristic.png}
  \caption{
    \small  
    \textbf{(i)} In $areaA \cup areaA'$, $t_1$ is the nearest to $a$;
    \textbf{(ii)} in $areaB \cup areaB'$, $t_5$ is the nearest to $b$;
    \textbf{(iii)} in $areaC$, $t_3$ is the nearest to $r$;
    \textbf{(iv)} in $areaC'$, $t_7$ is the nearest to $r'$; 
  }
  \label{fa}
\end{figure*}
\begin{itemize}
  \item Suppose the next nearest target $t$ is in $areaA \cup areaA'$ (equiv. $areaB \cup areaB'$).
    Then the optimal path must pass through the point $a$ (equiv. $b$) so the minimum $h$-value can be computed as:
    $\mathbf{min}\{d_e(r, a) + d_e(t, a)\}$ such that $t$ in $areaA \cup areaA'$
    (equiv. $areaB \cup areaB'$).
  \item Alternatively, suppose the next nearest target $t$ is instead in $areaC$.
    Then the optimal path must pass through a point $p$ in the open interval $(a, b)$.
    So the minimum $h$-value can be computed by minimising across all target points in $areaC$.
    A similar argument applies to a next nearest neighbour in $areaC'$ and we can apply the same strategy,
    but only after mirroring the root point $r$ through the interval.
    This operation is in contrast to the heuristic used by Polyanya in the point-to-point setting,
    which mirrors the target through the interval.
\end{itemize}
\noindent
Identifying the candidate target with minimum $h$-value in each of the four cases can be improved,
from a linear-time operation to NN query, by storing all of the
targets in a spatial data structure such as $R^*$-Tree~\cite{beckmann1990r}.
Thus we may compute a lower-bound estimate to the next nearest neighbour by minimising over
four candidates returned by the $R^*$-tree instead of evaluating all possible target points.

\subsection{Further Refinements}
We may notice that the $h_t$ described thus far is potentially costly,
when compared to one constant time operation in $h_p$.
To mitigate this we could call $h_t$ less often. 
An observation is that a parent search node and its successor may use same closest target $t$
in their $h_t$. In this case, instead of running a new query, the successor can directly
inherit the $t$ from the parent.  We call this strategy \textit{lazy compute} and apply it throughout our experiments.
We find it reduces total number of node expansion by approximately 15\%.

\begin{corollary}\label{lazy-compute}
  Given search node $n=(I,r)$, its successor $n'=(I', r')$,
  and a target $t$ which is the closest target of $n$ among all possible candidates.
  Further suppose $\textit{f-value}(n)=\textit{f-value}(n')$ .
  Then $t$ is also the closest target of $n'$ among all possible candidates.
\end{corollary}

\begin{proof}
  If there is $t'$ closer to $n'$ than $t$, then $h_p(n', t') <
  h_p(n', t) = h_p(n, t)$, then 
  $$
  \textit{g-value}(r) + d_e(r, r') + h_p(n', t') < \textit{g-value}(r) + d_e(r, r') + h_p(n',
  t) = \textit{f-value}(n)
  $$
  so that $\textit{f-value}(n') < \textit{f-value}(n)$, which conflict with
  Theorem~\ref{nodescv}. Thus, such $t'$ doesn't exist.
  %$\square$
\end{proof}

Now, each search node has a target, and the search behavior should be broadly similar to 
the point-to-point setting.  But there is one significant difference:
when a nearest neighbor $t$ has been found, $t$ should no longer influence the search process.
Thus, we need to remove $t$ from search space and re-assign (i.e. update)
all search nodes in the queue which use $t$ as their closest target. 
To avoid exploring the entire queue we propose instead the following simple strategy: 
when such a node is dequeud from the open list, we apply $h_t$ to compute a new target
and we push the node back onto open all without generating any successors. 
We call this \textit{lazy reassign}.

\begin{lemma}\label{lazy-reassign}
  Let's call these search nodes who need reassignment \textbf{pseudo nodes}, and others
  \textbf{real nodes}. \textbf{Lazy reassign} never changes the relative order of real
  nodes in queue.
\end{lemma}

\begin{proof}
  Let $n_1, n_2$ be any pair of real nodes, and $s$ be any pseudo node.  After the reassignment,
  if $s$ become neither $n_1$ nor $n_2$, then inserting a third party search node has nothing to do with the relative order of $n_1$
  and $n_2$; otherwise, without the loss of generality, assume $s$ becomes $n_1$. If the relative
  order of them is $<n_2, s>$, then $\textit{f-value}(n_2) <= \textit{f-value}(s)$, and
  because of the definition~\ref{close}, we have $\textit{f-value}(s) <= \textit{f-value}(n_1)$, so
  relative order of $n_1, n_2$ doesn't change. Alternatively, if the relative order is $<s, n_2>$,
  then $n_1$  will be push to queue before $n_2$ pop out, so the relative order of $n_1, n_2$ doesn't
  change as well.%$\square$
\end{proof}

In Algorithm~\ref{hsearch}, we arrive at last at the final form of Polyanya OkNN. The algorithm
accepts either $h_v$ and $h_t$ as a heuristic function and has, in both cases, the same high level steps.

\begin{algorithm}[!ht]
  \input{./code/hsearch.pseudo}
  \caption{Polyanya OkNN}
  \label{hsearch}
\end{algorithm}

\section{Summary}
In this chapter, we start with two straightforward adaptions of point-to-point Polyany
and finally proposed two heuristics for the Polyanya OkNN algorithm.
For \textit{Interval Heuristic}, it supports multi-target by
removing $t$ from \textit{h-value}; for \textit{Target Heuristic}, it supports multi-target by
employing spatial index and computing closest target dynamically.
%Each heuristic has their
%suitable scenario, in following chapters, we will evaluate their performance under different
%scenarios (chapter~\ref{empirical})  and discuss some future improvements
%(chapter~\ref{conclusionfuture}). 
